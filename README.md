# ğŸ“¦ Supply Chain Analytics Pipeline (Azure + Databricks + Streamlit)

This project demonstrates how to build a modern, scalable supply chain analytics pipeline using **Azure Blob Storage**, **Databricks (Community Edition)**, and **Streamlit**.

---

## ğŸšš Use Case
Simulates a realistic supply chain scenario involving:
- ğŸ“¦ Order fulfillment
- ğŸš› Shipment tracking & delay monitoring
- ğŸ¬ Inventory level monitoring
- âš ï¸ Low-stock alerting

---

## ğŸ› ï¸ Tech Stack
| Layer          | Tool                         | Purpose                                 |
|----------------|------------------------------|------------------------------------------|
| Ingest         | Azure Blob Storage           | Raw and processed data storage           |
| Orchestrate    | Azure Data Factory (via UI)  | Manages the pipeline workflow            |
| Transform      | Databricks (PySpark)         | ETL pipeline for joining, cleaning data  |
| Visualize      | Streamlit                    | Lightweight interactive dashboard        |
| Dev/Test       | Local CSVs in `data/`        | Used for offline development             |

---

## ğŸ” Workflow Overview
```text
Azure Blob Storage (Raw CSVs)
        â†“
Azure Data Factory (Orchestration)
        â†“
Databricks (ETL with PySpark)
        â†“
Azure Blob Storage (Processed Parquet/CSV)
        â†“
Streamlit Dashboard (Interactive Visualization)
```

---

## ğŸ¥š Components

### 1. Ingestion
Raw CSVs are uploaded to the `raw` container in Azure Blob Storage. Example data includes:
- `orders.csv`
- `inventory.csv`
- `shipments.csv`

### 2. Transformation (ETL)
Databricks reads from Azure Blob, cleans and joins the datasets:
- Parses delivery dates
- Joins orders with shipments
- Calculates delivery delays
- Writes output back to Blob (as Parquet or CSV)

### 3. Visualization
Streamlit reads the processed files from Azure Blob or local `data/curated/` and displays:
- Shipment delay histograms
- Inventory levels per warehouse
- KPI metrics for total orders/shipments

### 4. Orchestration (ADF)
Azure Data Factory orchestrates the pipeline. ADF setup is done via Azure Portal, and the template is exported as JSON for reproducibility.

---

## ğŸ“Š Streamlit Dashboard Features
- ğŸ“¦ Inventory Status by warehouse
- ğŸšš Shipment Delay distribution
- ğŸ“ˆ Order Trends (Coming soon)
- âš ï¸ Low Inventory Alerts (table view)

---

## ğŸ”— Architecture Diagram
![Architecture](architecture_diagram.png)

---

## ğŸš€ Getting Started
1. Upload your raw CSVs to Azure Blob Storage (container `raw`)
2. Run the PySpark notebook in Databricks
3. Output files will be saved to Blob (container `exports`)
4. Run the Streamlit dashboard locally or deploy

---

## ğŸ“ Notes
- This version uses **Azure Blob Storage as the clean data store**, not Azure SQL
- Azure Data Factory was configured via the **Azure Portal**, not fully automated yet
- `.devops/azure-pipelines.yml` is a placeholder for future CI/CD automation

---

## ğŸ“Œ License
MIT License

---




