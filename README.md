# ğŸ“¦ Supply Chain Analytics Pipeline (Azure + Databricks + Streamlit)

This project demonstrates how to build a modern, scalable supply chain analytics pipeline using **Azure Blob Storage**, **Databricks (Community Edition)**, and **Streamlit**.

---

## ğŸšš Use Case
Simulates a realistic supply chain scenario involving:
- ğŸ“¦ Order fulfillment
- ğŸš› Shipment tracking & delay monitoring
- ğŸ¬ Inventory level monitoring
- âš ï¸ Low-stock alerting

---

## ğŸ› ï¸ Tech Stack
| Layer          | Tool                         | Purpose                                 |
|----------------|------------------------------|------------------------------------------|
| Ingest         | Azure Blob Storage           | Raw and processed data storage           |
| Orchestrate    | Azure Data Factory (via UI)  | Manages the pipeline workflow            |
| Transform      | Databricks (PySpark)         | ETL pipeline for joining, cleaning data  |
| Visualize      | Streamlit                    | Lightweight interactive dashboard        |
| Dev/Test       | Local CSVs in `data/`        | Used for offline development             |

---

## ğŸ” Workflow Overview
```text
Azure Blob Storage (Raw CSVs)
        â†“
Azure Data Factory (Orchestration)
        â†“
Databricks (ETL with PySpark)
        â†“
Azure Blob Storage (Processed Parquet/CSV)
        â†“
Streamlit Dashboard (Interactive Visualization)
```

---

## ğŸ“‚ Project Structure
```
supply-chain-demo/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Simulated raw data (orders, inventory, shipments)
â”‚   â””â”€â”€ curated/               # Cleaned output files from ETL (for local Streamlit use)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ etl_pipeline.py        # PySpark ETL notebook
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ dashboard.py           # Streamlit dashboard (reads from Azure Blob or local CSVs)
â”œâ”€â”€ azure/
â”‚   â”œâ”€â”€ adf_pipeline.json      # Azure Data Factory pipeline export (via portal)
â”‚   â””â”€â”€ adf_pipeline.md        # Description of ADF configuration
â”œâ”€â”€ .devops/
â”‚   â””â”€â”€ azure-pipelines.yml    # CI/CD config placeholder (optional)
â”œâ”€â”€ architecture_diagram.png   # Updated architecture image (no Azure SQL)
â””â”€â”€ README.md
```

---

## ğŸ¥š Components

### 1. Ingestion
Raw CSVs are uploaded to the `raw` container in Azure Blob Storage. Example data includes:
- `orders.csv`
- `inventory.csv`
- `shipments.csv`

### 2. Transformation (ETL)
Databricks reads from Azure Blob, cleans and joins the datasets:
- Parses delivery dates
- Joins orders with shipments
- Calculates delivery delays
- Writes output back to Blob (as Parquet or CSV)

### 3. Visualization
Streamlit reads the processed files from Azure Blob or local `data/curated/` and displays:
- Shipment delay histograms
- Inventory levels per warehouse
- KPI metrics for total orders/shipments

### 4. Orchestration (ADF)
Azure Data Factory orchestrates the pipeline. ADF setup is done via Azure Portal, and the template is exported as JSON for reproducibility.

---

## ğŸ“Š Streamlit Dashboard Features
- ğŸ“¦ Inventory Status by warehouse
- ğŸšš Shipment Delay distribution
- ğŸ“ˆ Order Trends (Coming soon)
- âš ï¸ Low Inventory Alerts (table view)

---

## ğŸ”— Architecture Diagram
![Architecture](architecture_diagram.png)

---

## ğŸš€ Getting Started
1. Upload your raw CSVs to Azure Blob Storage (container `raw`)
2. Run the PySpark notebook in Databricks
3. Output files will be saved to Blob (container `exports`)
4. Run the Streamlit dashboard locally or deploy

---

## ğŸ“ Notes
- This version uses **Azure Blob Storage as the clean data store**, not Azure SQL
- Azure Data Factory was configured via the **Azure Portal**, not fully automated yet
- `.devops/azure-pipelines.yml` is a placeholder for future CI/CD automation

---

## ğŸ“Œ License
MIT License

---




